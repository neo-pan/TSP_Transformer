{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.10 64-bit ('longformer': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"train_tsp_reformer_TSP50.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","interpreter":{"hash":"b09abb5848c32be6deb48fe051633414a6004be5831cbafd372ccdb694fbf12e"}},"cells":[{"cell_type":"markdown","source":["## The Transformer Network for the Traveling Salesman Problem\n","\n","Xavier Bresson, Thomas Laurent, Feb 2021<br>\n","\n","Arxiv : https://arxiv.org/pdf/2103.03012.pdf<br>\n","Talk : https://ipam.wistia.com/medias/0jrweluovs<br>\n","Slides : https://t.co/ySxGiKtQL5<br>\n","\n","This code trains the transformer network by reinforcement learning.<br>\n","Use the beam search code to test the trained network.\n"],"metadata":{"id":"4dpbZ9zLNqIJ"}},{"cell_type":"code","execution_count":1,"source":["###################\n","# Libs\n","###################\n","\n","import torch\n","import torch.nn as nn\n","import time\n","import argparse\n","\n","import os\n","import datetime\n","\n","from torch.distributions.categorical import Categorical\n","from transformers import ReformerModel,ReformerConfig\n","from transformers.models.reformer.modeling_reformer import ReformerEncoder\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.checkpoint import checkpoint\n","# visualization \n","%matplotlib inline\n","from IPython.display import set_matplotlib_formats, clear_output\n","set_matplotlib_formats('png2x','pdf')\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","try: \n","    import networkx as nx\n","    from scipy.spatial.distance import pdist, squareform\n","    from concorde.tsp import TSPSolver # !pip install -e pyconcorde\n","except:\n","    pass\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n"],"outputs":[],"metadata":{"id":"lRh9RPLiNqIO","executionInfo":{"status":"ok","timestamp":1629095625346,"user_tz":-480,"elapsed":4103,"user":{"displayName":"jinxi Ding","photoUrl":"","userId":"17590626235145844507"}}}},{"cell_type":"code","execution_count":2,"source":["###################\n","# Hardware : CPU / GPU(s)\n","###################\n","\n","device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n","\n","gpu_id = '0' # select a single GPU  \n","#gpu_id = '2,3' # select multiple GPUs  \n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n","    \n","print(device)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["GPU name: GeForce RTX 2080, gpu_id: 0\n","cuda\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wV_gfFLNqIP","executionInfo":{"status":"ok","timestamp":1629095630213,"user_tz":-480,"elapsed":529,"user":{"displayName":"jinxi Ding","photoUrl":"","userId":"17590626235145844507"}},"outputId":"f481d8c5-ada0-461e-a205-14364e63b7c5"}},{"cell_type":"code","execution_count":3,"source":["###################\n","# Hyper-parameters\n","###################\n","\n","class DotDict(dict):\n","    def __init__(self, **kwds):\n","        self.update(kwds)\n","        self.__dict__ = self\n","        \n","args = DotDict()\n","# args.nb_nodes = 20 # TSP20\n","args.nb_nodes = 50 # TSP50\n","#args.nb_nodes = 100 # TSP100\n","# args.nb_nodes = 1000 # TSP1000\n","args.bsz = 512 # TSP20 TSP50\n","args.dim_emb = 128\n","args.dim_ff = 512\n","args.dim_input_nodes = 2\n","args.nb_layers_encoder = 6\n","args.nb_layers_decoder = 2\n","args.nb_heads = 8\n","args.nb_epochs = 10000\n","args.nb_batch_per_epoch = 250\n","args.nb_batch_eval = 20\n","args.gpu_id = gpu_id\n","args.lr = 1e-4\n","args.tol = 1e-3\n","args.batchnorm = True  # if batchnorm=True  than batch norm is used\n","#args.batchnorm = False # if batchnorm=False than layer norm is used\n","args.max_len_PE = 2000\n","# use automatic mixed precision\n","args.use_amp = True\n","print(args)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'nb_nodes': 50, 'bsz': 512, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 250, 'nb_batch_eval': 20, 'gpu_id': '0', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 2000, 'use_amp': True}\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0RjB7HDANqIP","executionInfo":{"status":"ok","timestamp":1629095670452,"user_tz":-480,"elapsed":547,"user":{"displayName":"jinxi Ding","photoUrl":"","userId":"17590626235145844507"}},"outputId":"2a785232-8800-4943-cc74-e08856dac60e"}},{"cell_type":"code","execution_count":4,"source":["\n","###################\n","# Network definition\n","# Notation : \n","#            bsz : batch size\n","#            nb_nodes : number of nodes/cities\n","#            dim_emb : embedding/hidden dimension\n","#            nb_heads : nb of attention heads\n","#            dim_ff : feed-forward dimension\n","#            nb_layers : number of encoder/decoder layers\n","###################\n","def compute_tour_length(x, tour): \n","    \"\"\"\n","    Compute the length of a batch of tours\n","    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n","             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n","    Output : L of size (bsz,)             batch of lengths of each tsp tour\n","    \"\"\"\n","    bsz = x.shape[0]\n","    nb_nodes = x.shape[1]\n","    arange_vec = torch.arange(bsz, device=x.device)\n","    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n","    previous_cities = first_cities\n","    L = torch.zeros(bsz, device=x.device)\n","    with torch.no_grad():\n","        for i in range(1,nb_nodes):\n","            current_cities = x[arange_vec, tour[:,i], :] \n","            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n","            previous_cities = current_cities\n","        L += torch.sum( (current_cities - first_cities)**2 , dim=1 )**0.5 # dist(last, first node)  \n","    return L\n","\n","\n","class Transformer_encoder_net(nn.Module):\n","    \"\"\"\n","    Encoder network based on self-attention transformer\n","    Inputs :  \n","      h of size      (bsz, nb_nodes+1, dim_emb)    batch of input cities\n","    Outputs :  \n","      h of size      (bsz, nb_nodes+1, dim_emb)    batch of encoded cities\n","      score of size  (bsz, nb_nodes+1, nb_nodes+1) batch of attention scores\n","    \"\"\"\n","    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n","        super(Transformer_encoder_net, self).__init__()\n","        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n","        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n","        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n","        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n","        if batchnorm:\n","            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n","            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n","        else:\n","            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n","            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n","        self.nb_layers = nb_layers\n","        self.nb_heads = nb_heads\n","        self.batchnorm = batchnorm\n","        \n","    def forward(self, h):      \n","        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n","        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n","        # L layers\n","        for i in range(self.nb_layers):\n","            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n","            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n","            # add residual connection\n","            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n","            if self.batchnorm:\n","                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n","                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n","            else:\n","                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n","            # feedforward\n","            h_rc = h # residual connection\n","            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n","            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n","            if self.batchnorm:\n","                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n","            else:\n","                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n","        # Transpose h\n","        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n","        return h, score\n","    \n","\n","def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n","    \"\"\"\n","    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n","      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n","    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n","    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n","             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n","             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n","             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n","             clip_value is a scalar \n","    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n","              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n","    \"\"\"\n","    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n","    if nb_heads>1:\n","        # PyTorch view requires contiguous dimensions for correct reshaping\n","        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n","        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n","        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n","        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n","        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n","        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n","        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n","        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n","        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n","    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n","    del K,Q\n","    if clip_value is not None:\n","        attn_weights = clip_value * torch.tanh(attn_weights)\n","    if mask is not None:\n","        if nb_heads>1:\n","            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n","        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n","        # attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-1e9')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n","        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), torch.finfo(attn_weights.dtype).min)\n","    attn_weights = torch.softmax(attn_weights, dim=-1) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n","    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n","    if nb_heads>1:\n","        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n","        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n","        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n","        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n","        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n","\n","        # attn_output = attn_output.transpose(1,2).contiguous().view(bsz, emd_dim, 1).transpose(1,2).contiguous()\n","        # attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes).mean(dim=1)\n","    return attn_output, attn_weights\n","    \n","\n","\n","\n","\n","\n","class AutoRegressiveDecoderLayer(nn.Module):\n","    \"\"\"\n","    Single decoder layer based on self-attention and query-attention\n","    Inputs :  \n","      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n","      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n","      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n","      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n","    Output :  \n","      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n","    \"\"\"\n","    def __init__(self, dim_emb, nb_heads):\n","        super(AutoRegressiveDecoderLayer, self).__init__()\n","        self.dim_emb = dim_emb\n","        self.nb_heads = nb_heads\n","        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n","        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n","        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n","        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n","        self.W0_att = nn.Linear(dim_emb, dim_emb)\n","        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n","        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n","        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n","        self.BN_selfatt = nn.LayerNorm(dim_emb)\n","        self.BN_att = nn.LayerNorm(dim_emb)\n","        self.BN_MLP = nn.LayerNorm(dim_emb)\n","        self.K_sa = None\n","        self.V_sa = None\n","        # self.MHA=nn.MultiheadAttention(embed_dim=dim_emb,num_heads=self.nb_heads,batch_first=True)\n","\n","    def reset_selfatt_keys_values(self):\n","        self.K_sa = None\n","        self.V_sa = None\n","        \n","    def update_k_v(self, h_t):\n","        bsz = h_t.size(0)\n","        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n","        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n","        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n","        if self.K_sa is None:\n","            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n","            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n","        else:\n","            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n","            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n","\n","    def forward(self, h_t, K_att, V_att, mask):\n","        bsz = h_t.size(0)\n","        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n","        # embed the query for self-attention\n","        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n","        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n","        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n","        # concatenate the new self-attention key and value to the previous keys and values\n","        if self.K_sa is None:\n","            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n","            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n","        else:\n","            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n","            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n","        # compute self-attention between nodes in the partial tour\n","        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n","        # h_t = h_t + self.W0_selfatt(self.MHA(q_sa, self.K_sa, self.V_sa)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n","        \n","        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n","        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n","        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n","        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n","        # mask = torch.repeat_interleave(mask, repeats=self.nb_heads, dim=0)\n","        h_t = h_t + self.W0_att(myMHA(q_a, K_att, V_att, self.nb_heads,mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n","        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n","        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n","        # MLP\n","        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n","        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n","        return h_t\n","        \n","        \n","class Transformer_decoder_net(nn.Module): \n","    \"\"\"\n","    Decoder network based on self-attention and query-attention transformers\n","    Inputs :  \n","      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n","      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n","      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n","      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n","    Output :  \n","      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n","    \"\"\"\n","    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n","        super(Transformer_decoder_net, self).__init__()\n","        self.dim_emb = dim_emb\n","        self.nb_heads = nb_heads\n","        self.nb_layers_decoder = nb_layers_decoder\n","        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )\n","        \n","        \n","        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n","        self.rnn_cell = nn.LSTMCell(dim_emb, dim_emb)\n","        \n","    # Reset to None self-attention keys and values when decoding starts \n","    def reset_selfatt_keys_values(self): \n","        for l in range(self.nb_layers_decoder-1):\n","            self.decoder_layers[l].reset_selfatt_keys_values()\n","            \n","    def forward(self, h_t, K_att, V_att, mask ,step,idx):\n","        for l in range(self.nb_layers_decoder):\n","            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb)\n","            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb)\n","            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers)\n","                if step==0:\n","                  h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n","                  self.hx_before = h_t\n","                  self.cx_before = h_t\n","                else:\n","                  self.decoder_layers[l].update_k_v(h_t)\n","                  # rnn_input = K_att_l[:,idx]\n","                  self.hx_before, self.cx_before = self.rnn_cell(h_t,(self.hx_before, self.cx_before))\n","                  h_t = self.hx_before\n","            else: # decoder layers with single head (final layer)\n","                q_final = self.Wq_final(h_t)\n","                bsz = h_t.size(0)\n","                q_final = q_final.view(bsz, 1, self.dim_emb)\n","                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1]\n","\n","        prob_next_node = attn_weights.squeeze(1) \n","        return prob_next_node\n","\n","    # def forward(self, h_t,K_att_l,V_att_l,mask):\n","    #     mask=[mask]*self.config.num_hidden_layers\n","    #     decoder_outputs=self.decoder_layers(h_t,attention_mask=mask)\n","    #     h_t = decoder_outputs.all_hidden_states[-1]\n","    #     q_final = self.Wq_final(h_t)\n","    #     bsz = h_t.size(0)\n","    #     q_final = q_final.view(bsz, 1, self.dim_emb)\n","    #     attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n","\n","    #     prob_next_node = attn_weights.squeeze(1) \n","    #     return prob_next_node\n","\n","\n","def generate_positional_encoding(d_model, max_len):\n","    \"\"\"\n","    Create standard transformer PEs.\n","    Inputs :  \n","      d_model is a scalar correspoding to the hidden dimension\n","      max_len is the maximum length of the sequence\n","    Output :  \n","      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n","    \"\"\"\n","    pe = torch.zeros(max_len, d_model)\n","    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","    pe[:,0::2] = torch.sin(position * div_term)\n","    pe[:,1::2] = torch.cos(position * div_term)\n","    return pe\n","    \n","    \n","class TSP_net(nn.Module): \n","    \"\"\"\n","    The TSP network is composed of two steps :\n","      Step 1. Encoder step : Take a set of 2D points representing a fully connected graph \n","                             and encode the set with self-transformer.\n","      Step 2. Decoder step : Build the TSP tour recursively/autoregressively, \n","                             i.e. one node at a time, with a self-transformer and query-transformer. \n","    Inputs : \n","      x of size (bsz, nb_nodes, dim_emb) Euclidian coordinates of the nodes/cities\n","      deterministic is a boolean : If True the salesman will chose the city with highest probability. \n","                                   If False the salesman will chose the city with Bernouilli sampling.\n","    Outputs : \n","      tours of size (bsz, nb_nodes) : batch of tours, i.e. sequences of ordered cities \n","                                      tours[b,t] contains the idx of the city visited at step t in batch b\n","      sumLogProbOfActions of size (bsz,) : batch of sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n","    \"\"\"\n","    \n","    def __init__(self, dim_input_nodes, dim_emb, dim_ff, nb_layers_encoder, nb_layers_decoder, nb_heads, max_len_PE,\n","                 batchnorm=True):\n","        super(TSP_net, self).__init__()\n","        \n","        self.dim_emb = dim_emb\n","        \n","        # input embedding layer\n","        self.input_emb = nn.Linear(dim_input_nodes, dim_emb)\n","        \n","        # encoder layer\n","        # self.encoder = Transformer_encoder_net(nb_layers_encoder, dim_emb, nb_heads, dim_ff, batchnorm)\n","        \n","        self.configuration = ReformerConfig()\n","        self.configuration.hidden_size=128\n","        self.configuration.num_attention_heads=8\n","        self.configuration.attention_head_size=16\n","        self.configuration.feed_forward_size=512\n","        self.configuration.local_attn_chunk_length=3200\n","        self.configuration.lsh_attn_chunk_length=3200\n","        self.configuration.num_buckets=2\n","        # print(self.configuration)\n","        self.encoder=ReformerEncoder(self.configuration)\n","        # vector to start decoding \n","        self.start_placeholder = nn.Parameter(torch.randn(dim_emb))\n","        \n","        # decoder layer\n","        self.decoder = Transformer_decoder_net(dim_emb, nb_heads, nb_layers_decoder)\n","        self.WK_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n","        self.WV_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n","        self.PE = generate_positional_encoding(dim_emb, max_len_PE)        \n","        \n","    def forward(self, x, deterministic=False):\n","\n","        # some parameters\n","        bsz = x.shape[0]\n","        nb_nodes = x.shape[1]\n","        zero_to_bsz = torch.arange(bsz, device=x.device) # [0,1,...,bsz-1]\n","\n","        # input embedding layer\n","        h = self.input_emb(x) # size(h)=(bsz, nb_nodes, dim_emb)\n","        \n","        # concat the nodes and the input placeholder that starts the decoding\n","        h = torch.cat([h, self.start_placeholder.repeat(bsz, 1, 1)], dim=1) # size(start_placeholder)=(bsz, nb_nodes+1, dim_emb)\n","        \n","        # encoder layer\n","        # h_encoder, _ = self.encoder(h) # size(h)=(bsz, nb_nodes+1, dim_emb)\n","\n","        # head_mask = get_head_mask(head_mask, self.configuration.num_hidden_layers, is_attention_chunked=True)\n","        head_mask = [None] * self.configuration.num_hidden_layers\n","        encoder_outputs = self.encoder(hidden_states=h,head_mask=head_mask,output_hidden_states =True )\n","        h_encoder = encoder_outputs.all_hidden_states[-1]\n","\n","        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n","        tours = []\n","\n","        # list that will contain Float tensors of shape (bsz,) that gives the neg log probs of the choices made at time t\n","        sumLogProbOfActions = []\n","\n","        # key and value for decoder    \n","        K_att_decoder = self.WK_att_decoder(h_encoder) # size(K_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n","        V_att_decoder = self.WV_att_decoder(h_encoder) # size(V_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n","        \n","        # input placeholder that starts the decoding\n","        self.PE = self.PE.to(x.device)\n","        idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n","        h_start = h_encoder[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz,1) # size(h_start)=(bsz, dim_emb)\n","        \n","        # initialize mask of visited cities\n","        mask_visited_nodes = torch.zeros(bsz, nb_nodes+1, device=x.device).bool() # False\n","        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n","        \n","        # clear key and val stored in the decoder\n","        self.decoder.reset_selfatt_keys_values()\n","\n","        # construct tour recursively\n","        h_t = h_start\n","        n_step=10\n","        id=0\n","        for t in range(nb_nodes):\n","            \n","            # compute probability over the next node in the tour\n","            prob_next_node = self.decoder(h_t, K_att_decoder, V_att_decoder, mask_visited_nodes,t%n_step,id) # size(prob_next_node)=(bsz, nb_nodes+1)\n","\n","            # choose node with highest probability or sample with Bernouilli \n","            if deterministic:\n","                idx = torch.argmax(prob_next_node, dim=1) # size(query)=(bsz,)\n","            else:\n","                idx = Categorical(prob_next_node).sample() # size(query)=(bsz,)\n","            \n","            # compute logprobs of the action items in the list sumLogProbOfActions   \n","            ProbOfChoices = prob_next_node[zero_to_bsz, idx] \n","            sumLogProbOfActions.append( torch.log(ProbOfChoices) )  # size(query)=(bsz,)\n","\n","            # update embedding of the current visited node\n","            h_t = h_encoder[zero_to_bsz, idx, :] # size(h_start)=(bsz, dim_emb)\n","            h_t = h_t + self.PE[t+1].expand(bsz, self.dim_emb)\n","            \n","            # update tour\n","            tours.append(idx)\n","\n","            # update masks with visited nodes\n","            mask_visited_nodes = mask_visited_nodes.clone()\n","            mask_visited_nodes[zero_to_bsz, idx] = True\n","            \n","            \n","        # logprob_of_choices = sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n","        sumLogProbOfActions = torch.stack(sumLogProbOfActions,dim=1).sum(dim=1) # size(sumLogProbOfActions)=(bsz,)\n","\n","        # convert the list of nodes into a tensor of shape (bsz,num_cities)\n","        tours = torch.stack(tours,dim=1) # size(col_index)=(bsz, nb_nodes)\n","        \n","        return tours, sumLogProbOfActions\n","    \n","    \n","    \n","\n","###################\n","# Instantiate a training network and a baseline network\n","###################\n","try: \n","    del model_train # remove existing model\n","    del model_baseline # remove existing model\n","except:\n","    pass\n","\n","model_train = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n","              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n","              batchnorm=args.batchnorm)\n","\n","model_baseline = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n","              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n","              batchnorm=args.batchnorm)\n","\n","# uncomment these lines if trained with multiple GPUs\n","print(torch.cuda.device_count())\n","if torch.cuda.device_count()>1:\n","    model_train = nn.DataParallel(model_train)\n","    model_baseline = nn.DataParallel(model_baseline)\n","# uncomment these lines if trained with multiple GPUs\n","\n","optimizer = torch.optim.Adam( model_train.parameters() , lr = args.lr ) \n","\n","model_train = model_train.to(device)\n","model_baseline = model_baseline.to(device)\n","model_baseline.eval()\n","\n","print(args); print('')\n","\n","# Logs\n","os.system(\"mkdir logs\")\n","time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n","file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n","file = open(file_name,\"w\",1) \n","file.write(time_stamp+'\\n\\n') \n","for arg in vars(args):\n","    file.write(arg)\n","    hyper_param_val=\"={}\".format(getattr(args, arg))\n","    file.write(hyper_param_val)\n","    file.write('\\n')\n","file.write('\\n\\n') \n","plot_performance_train = []\n","plot_performance_baseline = []\n","all_strings = []\n","epoch_ckpt = 0\n","tot_time_ckpt = 0\n","\n","\n","# # Uncomment these lines to re-start training with saved checkpoint\n","# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-25-00-n50-gpu0.pkl\"\n","# checkpoint = torch.load(checkpoint_file, map_location=device)\n","# epoch_ckpt = checkpoint['epoch'] + 1\n","# tot_time_ckpt = checkpoint['tot_time']\n","# plot_performance_train = checkpoint['plot_performance_train']\n","# plot_performance_baseline = checkpoint['plot_performance_baseline']\n","# model_baseline.load_state_dict(checkpoint['model_baseline'])\n","# model_train.load_state_dict(checkpoint['model_train'])\n","# optimizer.load_state_dict(checkpoint['optimizer'])\n","# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n","# del checkpoint\n","# # Uncomment these lines to re-start training with saved checkpoint\n","\n","\n","###################\n","# Main training loop \n","###################\n","start_training_time = time.time()\n","\n","scaler = GradScaler(enabled=args.use_amp)\n","for epoch in range(0,args.nb_epochs):\n","    \n","    # re-start training with saved checkpoint\n","    epoch += epoch_ckpt\n","\n","    ###################\n","    # Train model for one epoch\n","    ###################\n","    start = time.time()\n","    model_train.train() \n","\n","    for step in range(1,args.nb_batch_per_epoch+1):    \n","\n","        # generate a batch of random TSP instances    \n","        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(bsz, nb_nodes, dim_input_nodes) \n","        optimizer.zero_grad()\n","\n","        with autocast(enabled=args.use_amp):\n","          # compute tours for model\n","          tour_train, sumLogProbOfActions = model_train(x, deterministic=False) # size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\n","        \n","          # compute tours for baseline\n","          with torch.no_grad():\n","              tour_baseline, _ = model_baseline(x, deterministic=True)\n","\n","          # get the lengths of the tours\n","          L_train = compute_tour_length(x, tour_train) # size(L_train)=(bsz)\n","          L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(bsz)\n","          \n","          # backprop\n","          loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n","        \n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # scaler.scale(loss).backward()\n","        # scaler.step(optimizer)\n","        # scaler.update()\n","        \n","        # loss.backward()\n","        # optimizer.step()\n","        \n","    time_one_epoch = time.time()-start\n","    time_tot = time.time()-start_training_time + tot_time_ckpt\n","\n","        \n","    ###################\n","    # Evaluate train model and baseline on 10k random TSP instances\n","    ###################\n","    model_train.eval()\n","    mean_tour_length_train = 0\n","    mean_tour_length_baseline = 0\n","    for step in range(0,args.nb_batch_eval):\n","\n","        # generate a batch of random tsp instances   \n","        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n","\n","        # compute tour for model and baseline\n","        with torch.no_grad():\n","            tour_train, _ = model_train(x, deterministic=True)\n","            tour_baseline, _ = model_baseline(x, deterministic=True)\n","            \n","        # get the lengths of the tours\n","        L_train = compute_tour_length(x, tour_train)\n","        L_baseline = compute_tour_length(x, tour_baseline)\n","\n","        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n","        mean_tour_length_train += L_train.mean().item()\n","        mean_tour_length_baseline += L_baseline.mean().item()\n","\n","    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n","    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n","\n","    # evaluate train model and baseline and update if train model is better\n","    update_baseline = mean_tour_length_train+args.tol < mean_tour_length_baseline\n","    if update_baseline:\n","        model_baseline.load_state_dict( model_train.state_dict() )\n","    \n","    # For checkpoint\n","    plot_performance_train.append([ (epoch+1), mean_tour_length_train])\n","    plot_performance_baseline.append([ (epoch+1), mean_tour_length_baseline])\n","        \n","    # Compute optimality gap\n","    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692- 1.0\n","    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765- 1.0\n","    else: gap_train = -1.0\n","    \n","    # Print and save in txt file\n","    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n","        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, 100*gap_train, update_baseline) \n","    print(mystring_min) # Comment if plot display\n","    file.write(mystring_min+'\\n')\n","#     all_strings.append(mystring_min) # Uncomment if plot display\n","#     for string in all_strings: \n","#         print(string)\n","    \n","    # Saving checkpoint\n","    checkpoint_dir = os.path.join(\"checkpoint\")\n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","    torch.save({\n","        'epoch': epoch,\n","        'time': time_one_epoch,\n","        'tot_time': time_tot,\n","        'loss': loss.item(),\n","        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item()],\n","        'plot_performance_train': plot_performance_train,\n","        'plot_performance_baseline': plot_performance_baseline,\n","        'model_baseline': model_baseline.state_dict(),\n","        'model_train': model_train.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n","\n","       "],"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","{'nb_nodes': 50, 'bsz': 512, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 250, 'nb_batch_eval': 20, 'gpu_id': '0', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 2000, 'use_amp': True}\n","\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 2 and 3","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9172a424c2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m           \u001b[0;31m# compute tours for model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m           \u001b[0mtour_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumLogProbOfActions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m           \u001b[0;31m# compute tours for baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/envs/longformer/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9172a424c2a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# compute probability over the next node in the tour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mprob_next_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_att_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_att_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_visited_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# size(prob_next_node)=(bsz, nb_nodes+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# choose node with highest probability or sample with Bernouilli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/envs/longformer/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9172a424c2a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_t, K_att, V_att, mask, step, idx)\u001b[0m\n\u001b[1;32m    253\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_k_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                   \u001b[0;31m# rnn_input = K_att_l[:,idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9172a424c2a9>\u001b[0m in \u001b[0;36mupdate_k_v\u001b[0;34m(self, h_t)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_sa\u001b[0m \u001b[0;31m# size(self.V_sa)=(bsz, 1, dim_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_sa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_sa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"UF1pfd7mNqIQ","executionInfo":{"status":"error","timestamp":1629095962455,"user_tz":-480,"elapsed":36804,"user":{"displayName":"jinxi Ding","photoUrl":"","userId":"17590626235145844507"}},"outputId":"60bdae0d-e3cc-4437-8766-3ed372cfe80e"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"cxd4Oa-5jpmJ"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}