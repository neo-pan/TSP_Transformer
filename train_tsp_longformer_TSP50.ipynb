{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## The Transformer Network for the Traveling Salesman Problem\n",
    "\n",
    "Xavier Bresson, Thomas Laurent, Feb 2021<br>\n",
    "\n",
    "Arxiv : https://arxiv.org/pdf/2103.03012.pdf<br>\n",
    "Talk : https://ipam.wistia.com/medias/0jrweluovs<br>\n",
    "Slides : https://t.co/ySxGiKtQL5<br>\n",
    "\n",
    "This code trains the transformer network by reinforcement learning.<br>\n",
    "Use the beam search code to test the trained network.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "###################\n",
    "# Libs\n",
    "###################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "from longformer.longformer import LongformerSelfAttention\n",
    "from longformer.sliding_chunks import pad_to_window_size_3d\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: \n",
    "    import networkx as nx\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from concorde.tsp import TSPSolver # !pip install -e pyconcorde\n",
    "except:\n",
    "    pass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###################\n",
    "# Hardware : CPU / GPU(s)\n",
    "###################\n",
    "\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "gpu_id = '0' # select a single GPU  \n",
    "#gpu_id = '2,3' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "args = DotDict()\n",
    "# args.nb_nodes = 20 # TSP20\n",
    "args.nb_nodes = 50 # TSP50\n",
    "#args.nb_nodes = 100 # TSP100\n",
    "args.bsz = 512 # TSP20 TSP50\n",
    "args.dim_emb = 128\n",
    "args.dim_ff = 512\n",
    "args.dim_input_nodes = 2\n",
    "args.nb_layers_encoder = 6\n",
    "args.nb_layers_decoder = 2\n",
    "args.nb_heads = 8\n",
    "args.nb_epochs = 10000\n",
    "args.nb_batch_per_epoch = 2500\n",
    "args.nb_batch_eval = 20\n",
    "args.gpu_id = gpu_id\n",
    "args.lr = 1e-4\n",
    "args.tol = 1e-3\n",
    "args.batchnorm = True  # if batchnorm=True  than batch norm is used\n",
    "#args.batchnorm = False # if batchnorm=False than layer norm is used\n",
    "args.max_len_PE = 1000\n",
    "# args for Longformer\n",
    "args.attention_window_list = [25] * 6\n",
    "args.attention_window = max(args.attention_window_list)\n",
    "args.attention_dilation = 1\n",
    "# use automatic mixed precision\n",
    "args.use_amp = True\n",
    "# sort x as nearest neighbor\n",
    "args.sort_x = False\n",
    "\n",
    "print(args)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###################\n",
    "# Small test set for quick algorithm comparison\n",
    "# Note : this can be removed\n",
    "###################\n",
    "\n",
    "save_1000tsp = True\n",
    "save_1000tsp = False\n",
    "if save_1000tsp:\n",
    "    bsz = 1000\n",
    "    x = torch.rand(bsz, args.nb_nodes, args.dim_input_nodes, device='cpu') \n",
    "    print(x.size(),x[0])\n",
    "    data_dir = os.path.join(\"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if args.nb_nodes==20 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp20\"))\n",
    "    if args.nb_nodes==50 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp50\"))\n",
    "    if args.nb_nodes==100 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp100\"))\n",
    "\n",
    "checkpoint = None\n",
    "if args.nb_nodes==20 : checkpoint = torch.load(\"data/1000tsp20.pkl\")\n",
    "if args.nb_nodes==50 : checkpoint = torch.load(\"data/1000tsp50.pkl\")\n",
    "if args.nb_nodes==100 : checkpoint = torch.load(\"data/1000tsp100.pkl\")\n",
    "if checkpoint is not None:\n",
    "    x_1000tsp = checkpoint['x'].to(device)\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n",
    "else:\n",
    "    x_1000tsp = torch.rand(1000, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###################\n",
    "# Network definition\n",
    "# Notation : \n",
    "#            bsz : batch size\n",
    "#            nb_nodes : number of nodes/cities\n",
    "#            dim_emb : embedding/hidden dimension\n",
    "#            nb_heads : nb of attention heads\n",
    "#            dim_ff : feed-forward dimension\n",
    "#            nb_layers : number of encoder/decoder layers\n",
    "###################\n",
    "def compute_tour_length(x, tour): \n",
    "    \"\"\"\n",
    "    Compute the length of a batch of tours\n",
    "    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n",
    "             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n",
    "    Output : L of size (bsz,)             batch of lengths of each tsp tour\n",
    "    \"\"\"\n",
    "    bsz = x.shape[0]\n",
    "    nb_nodes = x.shape[1]\n",
    "    arange_vec = torch.arange(bsz, device=x.device)\n",
    "    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n",
    "    previous_cities = first_cities\n",
    "    L = torch.zeros(bsz, device=x.device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(1,nb_nodes):\n",
    "            current_cities = x[arange_vec, tour[:,i], :] \n",
    "            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n",
    "            previous_cities = current_cities\n",
    "        L += torch.sum( (current_cities - first_cities)**2 , dim=1 )**0.5 # dist(last, first node)  \n",
    "    return L\n",
    "\n",
    "@torch.no_grad()\n",
    "def tsp_nearest_neighbor(x):\n",
    "    '''Helper function.\n",
    "    Change  the random node sequence to a sorted nearest neighbor tour\n",
    "    '''\n",
    "    assert x.dim() == 3\n",
    "    assert x.size(2) == 2\n",
    "    bsz = x.size(0)\n",
    "    nb_nodes = x.size(1)\n",
    "    # compute pair-wise distance, dist_matrix.size()=(bsz, nb_nodes, nb_nodes)\n",
    "    dist_matrix = torch.cdist(x, x, p=2) \n",
    "    # mask selected nodes\n",
    "    mask = torch.zeros((bsz, nb_nodes), dtype=torch.bool, device=x.device)\n",
    "    # start from random node\n",
    "    random_begin = torch.randint(0, nb_nodes, (bsz,), device=x.device)\n",
    "    zero_to_bsz = torch.arange(0, bsz, device=x.device)\n",
    "\n",
    "    tour = []\n",
    "    idx = random_begin\n",
    "    while not mask.all():\n",
    "        # add index of selected node to list\n",
    "        tour.append(idx)\n",
    "        # mask selected node\n",
    "        mask[zero_to_bsz, idx] = 1\n",
    "        # mask distance matrix for selected node\n",
    "        dist = dist_matrix[zero_to_bsz, idx]\n",
    "        dist_mask = dist.masked_fill(mask, float(\"inf\"))\n",
    "        # select next nearest node\n",
    "        idx = dist_mask.argmin(dim=1)\n",
    "    assert len(tour) == nb_nodes\n",
    "    tour = torch.stack(tour, dim=1)\n",
    "    sorted_x = x.gather(dim=1, index=tour[..., None].expand_as(x))\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "class Transformer_encoder_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network based on self-attention transformer\n",
    "    Inputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of input cities\n",
    "    Outputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of encoded cities\n",
    "      score of size  (bsz, nb_nodes+1, nb_nodes+1) batch of attention scores\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(Transformer_encoder_net, self).__init__()\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n",
    "        self.MHA_layers = nn.ModuleList( [LongformerSelfAttention(dim_emb, nb_heads, args.attention_window_list[i]) for i in range(nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, h, attn_mask):\n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h = h.transpose(0,1)\n",
    "            h = self.MHA_layers[i](h, attn_mask)[0] # size(h)=(bsz, nb_nodes, dim_emb), size=None\n",
    "            h = h.transpose(0,1)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, None\n",
    "\n",
    "\n",
    "def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n",
    "    \"\"\"\n",
    "    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n",
    "      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n",
    "    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n",
    "    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n",
    "             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n",
    "             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n",
    "             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n",
    "             clip_value is a scalar \n",
    "    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n",
    "              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n",
    "    \"\"\"\n",
    "    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "    if nb_heads>1:\n",
    "        # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    if clip_value is not None:\n",
    "        attn_weights = clip_value * torch.tanh(attn_weights)\n",
    "    if mask is not None:\n",
    "        if nb_heads>1:\n",
    "            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n",
    "        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), torch.finfo(attn_weights.dtype).min) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights_float = torch.softmax(attn_weights, dim=-1, dtype=torch.float32) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = attn_weights_float.type_as(attn_weights)\n",
    "    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "    if nb_heads>1:\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n",
    "        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n",
    "    return attn_output, attn_weights\n",
    "    \n",
    "    \n",
    "class AutoRegressiveDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer based on self-attention and query-attention\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n",
    "      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n",
    "    Output :  \n",
    "      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads):\n",
    "        super(AutoRegressiveDecoderLayer, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.BN_selfatt = nn.LayerNorm(dim_emb)\n",
    "        self.BN_att = nn.LayerNorm(dim_emb)\n",
    "        self.BN_MLP = nn.LayerNorm(dim_emb)\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "\n",
    "    def reset_selfatt_keys_values(self):\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "        \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        bsz = h_t.size(0)\n",
    "        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # embed the query for self-attention\n",
    "        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n",
    "        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n",
    "        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n",
    "        # concatenate the new self-attention key and value to the previous keys and values\n",
    "        if self.K_sa is None:\n",
    "            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n",
    "            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n",
    "        else:\n",
    "            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n",
    "            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n",
    "        # compute self-attention between nodes in the partial tour\n",
    "        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n",
    "        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n",
    "        h_t = h_t + self.W0_att( myMHA(q_a, K_att, V_att, self.nb_heads, mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # MLP\n",
    "        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n",
    "        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n",
    "        return h_t\n",
    "        \n",
    "        \n",
    "class Transformer_decoder_net(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder network based on self-attention and query-attention transformers\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n",
    "      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n",
    "    Output :  \n",
    "      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n",
    "        super(Transformer_decoder_net, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.nb_layers_decoder = nb_layers_decoder\n",
    "        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )\n",
    "        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "    # Reset to None self-attention keys and values when decoding starts \n",
    "    def reset_selfatt_keys_values(self): \n",
    "        for l in range(self.nb_layers_decoder-1):\n",
    "            self.decoder_layers[l].reset_selfatt_keys_values()\n",
    "            \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        for l in range(self.nb_layers_decoder):\n",
    "            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers)\n",
    "                h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n",
    "            else: # decoder layers with single head (final layer)\n",
    "                q_final = self.Wq_final(h_t)\n",
    "                bsz = h_t.size(0)\n",
    "                q_final = q_final.view(bsz, 1, self.dim_emb)\n",
    "                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n",
    "        prob_next_node = attn_weights.squeeze(1) \n",
    "        return prob_next_node\n",
    "\n",
    "\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "    \n",
    "    \n",
    "class TSP_net(nn.Module): \n",
    "    \"\"\"\n",
    "    The TSP network is composed of two steps :\n",
    "      Step 1. Encoder step : Take a set of 2D points representing a fully connected graph \n",
    "                             and encode the set with self-transformer.\n",
    "      Step 2. Decoder step : Build the TSP tour recursively/autoregressively, \n",
    "                             i.e. one node at a time, with a self-transformer and query-transformer. \n",
    "    Inputs : \n",
    "      x of size (bsz, nb_nodes, dim_emb) Euclidian coordinates of the nodes/cities\n",
    "      deterministic is a boolean : If True the salesman will chose the city with highest probability. \n",
    "                                   If False the salesman will chose the city with Bernouilli sampling.\n",
    "    Outputs : \n",
    "      tours of size (bsz, nb_nodes) : batch of tours, i.e. sequences of ordered cities \n",
    "                                      tours[b,t] contains the idx of the city visited at step t in batch b\n",
    "      sumLogProbOfActions of size (bsz,) : batch of sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_input_nodes, dim_emb, dim_ff, nb_layers_encoder, nb_layers_decoder, nb_heads, max_len_PE,\n",
    "                 batchnorm=True):\n",
    "        super(TSP_net, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        \n",
    "        # input embedding layer\n",
    "        self.input_emb = nn.Linear(dim_input_nodes, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        self.encoder = Transformer_encoder_net(nb_layers_encoder, dim_emb, nb_heads, dim_ff, batchnorm)\n",
    "        \n",
    "        # vector to start decoding \n",
    "        self.start_placeholder = nn.Parameter(torch.randn(dim_emb))\n",
    "        \n",
    "        # decoder layer\n",
    "        self.decoder = Transformer_decoder_net(dim_emb, nb_heads, nb_layers_decoder)\n",
    "        # self.h_t_embed = nn.Linear(dim_emb * 2, dim_emb)\n",
    "        self.WK_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.WV_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.PE = generate_positional_encoding(dim_emb, max_len_PE)\n",
    "        \n",
    "    def forward(self, x, deterministic=False):\n",
    "\n",
    "        # some parameters\n",
    "        bsz = x.shape[0]\n",
    "        nb_nodes = x.shape[1]\n",
    "        zero_to_bsz = torch.arange(bsz, device=x.device) # [0,1,...,bsz-1]\n",
    "\n",
    "        # input embedding layer\n",
    "        h = self.input_emb(x) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        \n",
    "        # concat the nodes and the input placeholder that starts the decoding\n",
    "        h = torch.cat([h, self.start_placeholder.repeat(bsz, 1, 1)], dim=1) # size(h)=(bsz, nb_nodes+1, dim_emb)\n",
    "        attn_mask = torch.zeros((bsz, nb_nodes+1), dtype=torch.long, device=h.device)\n",
    "        # idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        # set virtual node for global attention\n",
    "        # attn_mask[zero_to_bsz, idx_start_placeholder] = 1\n",
    "\n",
    "        # LongformerSelfAttention need 4-dim attention mask\n",
    "        attn_mask = attn_mask[:, None, None, :]\n",
    "\n",
    "        # encoder layer\n",
    "        h_encoder, _ = self.encoder(h, attn_mask) # size(h)=(bsz, nb_nodes+1, dim_emb)\n",
    "\n",
    "        assert h_encoder.size(1) == nb_nodes+1, f\"{h_encoder.size(1)} == {nb_nodes+1}\"\n",
    "\n",
    "        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n",
    "        tours = []\n",
    "\n",
    "        # list that will contain Float tensors of shape (bsz,) that gives the neg log probs of the choices made at time t\n",
    "        sumLogProbOfActions = []\n",
    "\n",
    "        # key and value for decoder    \n",
    "        K_att_decoder = self.WK_att_decoder(h_encoder) # size(K_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        V_att_decoder = self.WV_att_decoder(h_encoder) # size(V_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        \n",
    "        # input placeholder that starts the decoding\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        # h_global = h_encoder[zero_to_bsz, idx_start_placeholder, :]\n",
    "        h_start = h_encoder[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz,1) # size(h_start)=(bsz, dim_emb)\n",
    "        \n",
    "        # initialize mask of visited cities\n",
    "        mask_visited_nodes = torch.zeros(bsz, nb_nodes+1, device=x.device).bool() # False\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "        \n",
    "        # clear key and val stored in the decoder\n",
    "        self.decoder.reset_selfatt_keys_values()\n",
    "\n",
    "        # construct tour recursively\n",
    "        h_t = h_start\n",
    "        for t in range(nb_nodes):\n",
    "            # h_t = self.h_t_embed(torch.cat([h_global, h_t], dim=-1))\n",
    "            # compute probability over the next node in the tour\n",
    "            prob_next_node = self.decoder(h_t, K_att_decoder, V_att_decoder, mask_visited_nodes) # size(prob_next_node)=(bsz, nb_nodes+1)\n",
    "            \n",
    "            # choose node with highest probability or sample with Bernouilli \n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1) # size(query)=(bsz,)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample() # size(query)=(bsz,)\n",
    "            \n",
    "            # compute logprobs of the action items in the list sumLogProbOfActions   \n",
    "            ProbOfChoices = prob_next_node[zero_to_bsz, idx] \n",
    "            sumLogProbOfActions.append( torch.log(ProbOfChoices.type(torch.float32)).type_as(ProbOfChoices) ) # size(query)=(bsz,)\n",
    "\n",
    "            # update embedding of the current visited node\n",
    "            h_t = h_encoder[zero_to_bsz, idx, :] # size(h_start)=(bsz, dim_emb)\n",
    "            h_t = h_t + self.PE[t+1].expand(bsz, self.dim_emb)\n",
    "            \n",
    "            # update tour\n",
    "            tours.append(idx)\n",
    "\n",
    "            # update masks with visited nodes\n",
    "            mask_visited_nodes = mask_visited_nodes.clone()\n",
    "            mask_visited_nodes[zero_to_bsz, idx] = True\n",
    "            \n",
    "            \n",
    "        # logprob_of_choices = sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "        sumLogProbOfActions = torch.stack(sumLogProbOfActions,dim=1).sum(dim=1) # size(sumLogProbOfActions)=(bsz,)\n",
    "\n",
    "        # convert the list of nodes into a tensor of shape (bsz,num_cities)\n",
    "        tours = torch.stack(tours,dim=1) # size(col_index)=(bsz, nb_nodes)\n",
    "        \n",
    "        return tours, sumLogProbOfActions\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "###################\n",
    "# Instantiate a training network and a baseline network\n",
    "###################\n",
    "try: \n",
    "    del model_train # remove existing model\n",
    "    del model_baseline # remove existing model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_train = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "model_baseline = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.device_count()>1:\n",
    "    model_train = nn.DataParallel(model_train)\n",
    "    model_baseline = nn.DataParallel(model_baseline)\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "\n",
    "optimizer = torch.optim.Adam( model_train.parameters() , lr = args.lr ) \n",
    "scaler = GradScaler(enabled=args.use_amp)\n",
    "\n",
    "model_train = model_train.to(device)\n",
    "model_baseline = model_baseline.to(device)\n",
    "model_baseline.eval()\n",
    "\n",
    "print(args); print('')\n",
    "\n",
    "# Logs\n",
    "os.system(\"mkdir logs\")\n",
    "time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "for arg in vars(args):\n",
    "    file.write(arg)\n",
    "    hyper_param_val=\"={}\".format(getattr(args, arg))\n",
    "    file.write(hyper_param_val)\n",
    "    file.write('\\n')\n",
    "file.write('\\n\\n') \n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "all_strings = []\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-25-00-n50-gpu0.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "# epoch_ckpt = checkpoint['epoch'] + 1\n",
    "# tot_time_ckpt = checkpoint['tot_time']\n",
    "# plot_performance_train = checkpoint['plot_performance_train']\n",
    "# plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "# model_baseline.load_state_dict(checkpoint['model_baseline'])\n",
    "# model_train.load_state_dict(checkpoint['model_train'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "# del checkpoint\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "\n",
    "\n",
    "###################\n",
    "# Main training loop \n",
    "###################\n",
    "start_training_time = time.time()\n",
    "\n",
    "for epoch in range(0,args.nb_epochs):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt\n",
    "\n",
    "    ###################\n",
    "    # Train model for one epoch\n",
    "    ###################\n",
    "    start = time.time()\n",
    "    model_train.train() \n",
    "\n",
    "    for step in tqdm(range(1,args.nb_batch_per_epoch+1)):    \n",
    "\n",
    "        # generate a batch of random TSP instances    \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(bsz, nb_nodes, dim_input_nodes) \n",
    "\n",
    "        # compute tours for model\n",
    "        with autocast(enabled=args.use_amp):\n",
    "            if args.sort_x:\n",
    "                x = tsp_nearest_neighbor(x)\n",
    "            tour_train, sumLogProbOfActions = model_train(x, deterministic=False) # size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\n",
    "      \n",
    "            # compute tours for baseline\n",
    "            with torch.no_grad():\n",
    "                tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "\n",
    "            # get the lengths of the tours\n",
    "            L_train = compute_tour_length(x, tour_train) # size(L_train)=(bsz)\n",
    "            L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(bsz)\n",
    "            \n",
    "            # backprop\n",
    "            loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "    time_one_epoch = time.time()-start\n",
    "    time_tot = time.time()-start_training_time + tot_time_ckpt\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # Evaluate train model and baseline on 10k random TSP instances\n",
    "    ###################\n",
    "    model_train.eval()\n",
    "    mean_tour_length_train = 0\n",
    "    mean_tour_length_baseline = 0\n",
    "    for step in range(0,args.nb_batch_eval):\n",
    "\n",
    "        # generate a batch of random tsp instances   \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n",
    "        if args.sort_x:\n",
    "            x = tsp_nearest_neighbor(x)\n",
    "\n",
    "        # compute tour for model and baseline\n",
    "        with torch.no_grad():\n",
    "            tour_train, _ = model_train(x, deterministic=True)\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "            \n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline)\n",
    "\n",
    "        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n",
    "        mean_tour_length_train += L_train.mean().item()\n",
    "        mean_tour_length_baseline += L_baseline.mean().item()\n",
    "\n",
    "    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n",
    "    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n",
    "\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "    update_baseline = mean_tour_length_train+args.tol < mean_tour_length_baseline\n",
    "    if update_baseline:\n",
    "        model_baseline.load_state_dict( model_train.state_dict() )\n",
    "\n",
    "    # Compute TSPs for small test set\n",
    "    # Note : this can be removed\n",
    "    with torch.no_grad():\n",
    "        tour_baseline, _ = model_baseline(x_1000tsp, deterministic=True)\n",
    "    mean_tour_length_test = compute_tour_length(x_1000tsp, tour_baseline).mean().item()\n",
    "    \n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([ (epoch+1), mean_tour_length_train])\n",
    "    plot_performance_baseline.append([ (epoch+1), mean_tour_length_baseline])\n",
    "        \n",
    "    # Compute optimality gap\n",
    "    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692- 1.0\n",
    "    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765- 1.0\n",
    "    else: gap_train = -1.0\n",
    "    \n",
    "    # Print and save in txt file\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline) \n",
    "    print(mystring_min) # Comment if plot display\n",
    "    file.write(mystring_min+'\\n')\n",
    "#     all_strings.append(mystring_min) # Uncomment if plot display\n",
    "#     for string in all_strings: \n",
    "#         print(string)\n",
    "    \n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item(), mean_tour_length_test],\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'mean_tour_length_test': mean_tour_length_test,\n",
    "        'model_baseline': model_baseline.state_dict(),\n",
    "        'model_train': model_train.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare local self attention and full self attention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# x = torch.arange(10).view((1,5,2)).float() / 10\n",
    "x = torch.rand(256,51,128)\n",
    "x = x.transpose(0,1)\n",
    "seq_len, bsz, embed_dim = x.size()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q = x.clone()\n",
    "k = x.clone()\n",
    "v = x.clone()\n",
    "num_heads = 8\n",
    "head_dim = embed_dim // num_heads\n",
    "attention_window = seq_len // 2\n",
    "attention_dilation = 1\n",
    "attention_mode  = \"sliding_chunks\"\n",
    "attention_mask = torch.zeros((bsz, seq_len)).long()[:, None, None, :]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "from longformer.sliding_chunks import circular_pad_mask, circular_pad_seq, sliding_chunks_matmul_qk, sliding_chunks_matmul_pv\n",
    "from longformer.diagonaled_mm_tvm import mask_invalid_locations\n",
    "\n",
    "q /= math.sqrt(head_dim)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_mask = attention_mask.squeeze(dim=2).squeeze(dim=1)\n",
    "    key_padding_mask = attention_mask < 0\n",
    "    extra_attention_mask = attention_mask > 0\n",
    "    remove_from_windowed_attention_mask = attention_mask != 0\n",
    "\n",
    "    num_extra_indices_per_batch = extra_attention_mask.long().sum(dim=1)\n",
    "    max_num_extra_indices_per_batch = num_extra_indices_per_batch.max()\n",
    "    if max_num_extra_indices_per_batch <= 0:\n",
    "        extra_attention_mask = None\n",
    "    else:\n",
    "        # To support the case of variable number of global attention in the rows of a batch,\n",
    "        # we use the following three selection masks to select global attention embeddings\n",
    "        # in a 3d tensor and pad it to `max_num_extra_indices_per_batch`\n",
    "        # 1) selecting embeddings that correspond to global attention\n",
    "        extra_attention_mask_nonzeros = extra_attention_mask.nonzero(as_tuple=True)\n",
    "        zero_to_max_range = torch.arange(0, max_num_extra_indices_per_batch,\n",
    "                                            device=num_extra_indices_per_batch.device)\n",
    "        # mask indicating which values are actually going to be padding\n",
    "        selection_padding_mask = zero_to_max_range < num_extra_indices_per_batch.unsqueeze(dim=-1)\n",
    "        # 2) location of the non-padding values in the selected global attention\n",
    "        selection_padding_mask_nonzeros = selection_padding_mask.nonzero(as_tuple=True)\n",
    "        # 3) location of the padding values in the selected global attention\n",
    "        selection_padding_mask_zeros = (selection_padding_mask == 0).nonzero(as_tuple=True)\n",
    "\n",
    "    circular_attention_mask = circular_pad_mask(attention_mask, attention_window)\n",
    "    circular_key_padding_mask = circular_attention_mask < 0\n",
    "    circular_remove_from_windowed_attention_mask = circular_attention_mask != 0\n",
    "else:\n",
    "    remove_from_windowed_attention_mask = None\n",
    "    extra_attention_mask = None\n",
    "    key_padding_mask = None\n",
    "    circular_key_padding_mask = None\n",
    "    circular_remove_from_windowed_attention_mask = None\n",
    "\n",
    "\n",
    "q = q.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "k = k.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "circular_q = circular_pad_seq(q, attention_window, 0)\n",
    "circular_k = circular_pad_seq(k, attention_window, 0)\n",
    "circular_seq_len = circular_q.size(1)\n",
    "# attn_weights = (bsz, seq_len, num_heads, window*2+1)\n",
    "if attention_mode == 'tvm':\n",
    "    raise NotImplementedError\n",
    "    q = q.float().contiguous()\n",
    "    k = k.float().contiguous()\n",
    "    attn_weights = diagonaled_mm_tvm(q, k, attention_window, attention_dilation, False, 0, False)\n",
    "elif attention_mode == \"sliding_chunks\":\n",
    "    # attn_weights.shape (bsz, num_heads, seqlen, 2 * w + 1)\n",
    "    circular_attn_weights = sliding_chunks_matmul_qk(circular_q, circular_k, attention_window, padding_value=0)\n",
    "elif attention_mode == \"sliding_chunks_no_overlap\":\n",
    "    raise NotImplementedError\n",
    "    attn_weights = sliding_chunks_no_overlap_matmul_qk(q, k, attention_window, padding_value=0)\n",
    "else:\n",
    "    raise False\n",
    "mask_invalid_locations(circular_attn_weights, attention_window, attention_dilation, False)\n",
    "if circular_remove_from_windowed_attention_mask is not None:\n",
    "    # This implementation is fast and takes very little memory because num_heads x hidden_size = 1\n",
    "    # from (bsz x seq_len) to (bsz x seq_len x num_heads x hidden_size)\n",
    "    circular_remove_from_windowed_attention_mask = circular_remove_from_windowed_attention_mask.unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "    # cast to float/half then replace 1's with -inf\n",
    "    float_mask = circular_remove_from_windowed_attention_mask.type_as(q).masked_fill(circular_remove_from_windowed_attention_mask, -10000.0)\n",
    "    repeat_size = 1\n",
    "    float_mask = float_mask.repeat(1, 1, repeat_size, 1)\n",
    "    ones = float_mask.new_ones(size=float_mask.size())  # tensor of ones\n",
    "    # diagonal mask with zeros everywhere and -inf inplace of padding\n",
    "    if attention_mode == 'tvm':\n",
    "        raise NotImplementedError\n",
    "        d_mask = diagonaled_mm_tvm(ones, float_mask, attention_window, attention_dilation, False, 0, False)\n",
    "    elif attention_mode == \"sliding_chunks\":\n",
    "        d_mask = sliding_chunks_matmul_qk(ones, float_mask, attention_window, padding_value=0)\n",
    "    elif attention_mode == \"sliding_chunks_no_overlap\":\n",
    "        raise NotImplementedError\n",
    "        d_mask = sliding_chunks_no_overlap_matmul_qk(ones, float_mask, attention_window, padding_value=0)\n",
    "\n",
    "    circular_attn_weights += d_mask\n",
    "assert list(circular_attn_weights.size())[:3] == [bsz, circular_seq_len, num_heads]\n",
    "assert circular_attn_weights.size(dim=3) in [attention_window * 2 + 1, attention_window * 3]\n",
    "\n",
    "# get valid attn_weights (bsz, seq_len, num_heads, window*2+1)\n",
    "attn_weights = circular_attn_weights[:, attention_window: attention_window+seq_len, ...]\n",
    "assert list(attn_weights.size()) == [bsz, seq_len, num_heads, attention_window * 2 + 1]\n",
    "\n",
    "# the extra attention\n",
    "# if extra_attention_mask is not None:\n",
    "#     selected_k = k.new_zeros(bsz, max_num_extra_indices_per_batch, num_heads, head_dim)\n",
    "#     selected_k[selection_padding_mask_nonzeros] = k[extra_attention_mask_nonzeros]\n",
    "#     # (bsz, seq_len, num_heads, max_num_extra_indices_per_batch)\n",
    "#     selected_attn_weights = torch.einsum('blhd,bshd->blhs', (q, selected_k))\n",
    "#     selected_attn_weights[selection_padding_mask_zeros[0], :, :, selection_padding_mask_zeros[1]] = -10000\n",
    "#     # concat to attn_weights\n",
    "#     # (bsz, seq_len, num_heads, extra attention count + 2*window+1)\n",
    "#     attn_weights = torch.cat((selected_attn_weights, attn_weights), dim=-1)\n",
    "attn_weights_float = F.softmax(attn_weights, dim=-1, dtype=torch.float32)  # use fp32 for numerical stability\n",
    "if key_padding_mask is not None:\n",
    "    # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n",
    "    attn_weights_float = torch.masked_fill(attn_weights_float, key_padding_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n",
    "attn_weights = attn_weights_float.type_as(attn_weights)\n",
    "attn_probs = attn_weights\n",
    "\n",
    "v = v.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "circular_v = circular_pad_seq(v, attention_window, 0)\n",
    "\n",
    "attn = 0\n",
    "# if extra_attention_mask is not None:\n",
    "#     print(\"do extra ccompute\")\n",
    "#     selected_attn_probs = attn_probs.narrow(-1, 0, max_num_extra_indices_per_batch)\n",
    "#     selected_v = v.new_zeros(bsz, max_num_extra_indices_per_batch, num_heads, head_dim)\n",
    "#     selected_v[selection_padding_mask_nonzeros] = v[extra_attention_mask_nonzeros]\n",
    "#     # use `matmul` because `einsum` crashes sometimes with fp16\n",
    "#     # attn = torch.einsum('blhs,bshd->blhd', (selected_attn_probs, selected_v))\n",
    "#     attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2).type_as(selected_attn_probs)).transpose(1, 2)\n",
    "#     attn_probs = attn_probs.narrow(-1, max_num_extra_indices_per_batch, attn_probs.size(-1) - max_num_extra_indices_per_batch).contiguous()\n",
    "\n",
    "assert list(attn_probs.size()) == [bsz, seq_len, num_heads, attention_window * 2 + 1]\n",
    "circular_attn_probs = circular_pad_seq(attn_probs, attention_window, 0)\n",
    "\n",
    "if attention_mode == 'tvm':\n",
    "    raise NotImplementedError\n",
    "    v = v.float().contiguous()\n",
    "    attn += diagonaled_mm_tvm(attn_probs, v, attention_window, attention_dilation, True, 0, False)\n",
    "elif attention_mode == \"sliding_chunks\":\n",
    "    # (bsz, circular_seq_len, num_heads, head_dim)\n",
    "    circular_attn= sliding_chunks_matmul_pv(circular_attn_probs, circular_v, attention_window)\n",
    "    attn += circular_attn[:, attention_window: attention_window+seq_len, ...]\n",
    "elif attention_mode == \"sliding_chunks_no_overlap\":\n",
    "    raise NotImplementedError\n",
    "    attn += sliding_chunks_no_overlap_matmul_pv(attn_probs, v, attention_window)\n",
    "else:\n",
    "    raise False\n",
    "\n",
    "attn = attn.type_as(q)\n",
    "assert list(attn.size()) == [bsz, seq_len, num_heads, head_dim]\n",
    "attn = attn.transpose(0, 1).reshape(seq_len, bsz, embed_dim).transpose(0, 1)\n",
    "\n",
    "local_attn_probs = attn_probs\n",
    "local_attn = attn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "local_attn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "q /= math.sqrt(head_dim)\n",
    "q = q.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "k = k.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "attn_weights = torch.einsum('blhd,bshd->blhs', (q, k))\n",
    "attn_weights_float = F.softmax(attn_weights, dim=-1, dtype=torch.float32)\n",
    "attn_weights = attn_weights_float.type_as(attn_weights)\n",
    "attn_probs = attn_weights\n",
    "v = v.view(seq_len, bsz, num_heads, head_dim).transpose(0, 1)\n",
    "attn = torch.einsum('blhs,bshd->blhd', (attn_probs, v))\n",
    "attn = attn.transpose(0, 1).reshape(seq_len, bsz, embed_dim).transpose(0, 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seq_len, bsz, embed_dim = x.size()\n",
    "\n",
    "Q = x.clone().transpose(0,1)\n",
    "K = x.clone().transpose(0,1)\n",
    "V = x.clone().transpose(0,1)\n",
    "\n",
    "nb_heads = 8\n",
    "head_dim = embed_dim // num_heads\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "if nb_heads>1:\n",
    "    # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "    Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "    Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "    Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "    K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "    K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "    K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "    V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "    V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "\n",
    "attn_weights_float = torch.softmax(attn_weights, dim=-1, dtype=torch.float32) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "attn_weights = attn_weights_float.type_as(attn_weights)\n",
    "attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "if nb_heads>1:\n",
    "    attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes)\n",
    "    attn_output = attn_output.view(bsz, emd_dim, nb_nodes) # size(attn_output)=(bsz, dim_emb, nb_nodes)\n",
    "    attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, nb_nodes, dim_emb)\n",
    "    attn_weights = attn_weights.view(bsz, nb_heads, nb_nodes, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.allclose(local_attn, attn_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare `LongformerSelfAttention` and `MultiheadAttention`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "x = torch.rand(256,51,128)\n",
    "y = torch.arange(51)[None, :, None].expand(256,51,128) / 51 + 0.5\n",
    "bsz, seq_len, embed_dim = x.size()\n",
    "attention_window = seq_len // 2\n",
    "assert attention_window*2+1 ==seq_len\n",
    "nb_heads = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "local_attention = LongformerSelfAttention(embed_dim, nb_heads, attention_window)\n",
    "full_attention = nn.MultiheadAttention(embed_dim, nb_heads, batch_first=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "from torch.nn.init import eye_, zeros_\n",
    "for module in [local_attention, full_attention]:\n",
    "    for k,v in module.state_dict().items():\n",
    "        if \"weight\" in k:\n",
    "            eye_(v)\n",
    "        if \"bias\" in k:\n",
    "            zeros_(v)\n",
    "\n",
    "eye_(full_attention.get_parameter(\"in_proj_weight\")[:embed_dim])\n",
    "eye_(full_attention.get_parameter(\"in_proj_weight\")[embed_dim:2*embed_dim])\n",
    "eye_(full_attention.get_parameter(\"in_proj_weight\")[2*embed_dim:])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "hidden_state = x\n",
    "attention_mask = torch.zeros((bsz, seq_len)).long()[:, None, None, :]\n",
    "opt1 = torch.optim.Adam( local_attention.parameters() , lr = 1e-4 )\n",
    "opt2 = torch.optim.Adam( full_attention.parameters() , lr = 1e-4 )\n",
    "loss_func = nn.MSELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "for i in range(10):\n",
    "    la = local_attention(hidden_state, attention_mask)[0]\n",
    "    loss = loss_func(la, y)\n",
    "    opt1.zero_grad()\n",
    "    loss.backward()\n",
    "    opt1.step()\n",
    "    print(f\"step {i}, loss {loss}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0, loss 0.31538864970207214\n",
      "step 1, loss 0.30272069573402405\n",
      "step 2, loss 0.29026100039482117\n",
      "step 3, loss 0.27802732586860657\n",
      "step 4, loss 0.2660377621650696\n",
      "step 5, loss 0.2543106973171234\n",
      "step 6, loss 0.242864727973938\n",
      "step 7, loss 0.23171848058700562\n",
      "step 8, loss 0.22089055180549622\n",
      "step 9, loss 0.210399329662323\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "for i in range(10):\n",
    "    fa = full_attention(hidden_state, hidden_state, hidden_state)[0]\n",
    "    loss = loss_func(fa, y)\n",
    "    opt2.zero_grad()\n",
    "    loss.backward()\n",
    "    opt2.step()\n",
    "    print(f\"step {i}, loss {loss}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 0, loss 0.31538864970207214\n",
      "step 1, loss 0.30272069573402405\n",
      "step 2, loss 0.29026103019714355\n",
      "step 3, loss 0.27802735567092896\n",
      "step 4, loss 0.2660377323627472\n",
      "step 5, loss 0.2543106973171234\n",
      "step 6, loss 0.242864727973938\n",
      "step 7, loss 0.23171848058700562\n",
      "step 8, loss 0.22089055180549622\n",
      "step 9, loss 0.2103993147611618\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "torch.allclose(la, fa)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('longformer': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "b09abb5848c32be6deb48fe051633414a6004be5831cbafd372ccdb694fbf12e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}